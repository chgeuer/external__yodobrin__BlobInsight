{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d367b10-6527-4c1f-aec1-865236e9b475",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Blob life cycle - Research notebook\n",
    "\n",
    "This notebook provide key steps to analyze your blob access patterns, with goal to create life cycle policies that would reduce the cost of storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Both data points are avilable in the same storage account used as your data lake.\n",
    "The data points are:\n",
    "\n",
    "- Blob Inventory snapshot - in parquet format\n",
    "- Blob diagnostic logs (Read file) \n",
    "\n",
    "## Connecting to your datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd46ecb7-9277-4501-afb3-fe67f560564c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Inventory storage\n",
    "\n",
    "invent_storage_account_name = \"<your data lake storage account>\"\n",
    "invent_storage_account_key = \"< your key>\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.{0}.dfs.core.windows.net\".format(invent_storage_account_name), invent_storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa87182-6ba5-438f-ad91-9484686230b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Diagnostic logs (Parquet) Files into DataFrame\n",
    "\n",
    "Reading all access logs copied via data factory into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6512032-662e-4706-8be1-01606ec6c062",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lake_container = 'inventorylake'\n",
    "# Set the path to your parquet log files, note that with 'y=2023/*/*/*/*/' you read all logs of 2023\n",
    "diag_log_path = 'diag_logs/resourceId=/subscriptions/11acf2e0-XXXX-XXXX-86ef-37c53a9XXXXX/resourceGroups/ext-lake/providers/Microsoft.Storage/storageAccounts/targetextsa/blobServices/default/y=2023/*/*/*/*/'\n",
    "# Define the path to your parquet files so spark can read them\n",
    "path = f\"abfss://{lake_container}@{invent_storage_account_name}.dfs.core.windows.net/{diag_log_path}\"\n",
    "\n",
    "# Read the parquet files into a DataFrame\n",
    "df = spark.read.parquet(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b9319b-391f-493e-adc0-d75a93dfb98d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Summary of activities for the entire period\n",
    "\n",
    "The ```groupBy``` operation provides quick overview of the diffrent operations in your storage account.\n",
    "\n",
    "Note, that for the access patterns we would need to focus on 'GetBlob' and 'ReadFile' operations. Using ```truncate=False``` will show the full length of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d99164-9310-4816-a3a0-1371f7a66ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|operationName              |count|\n",
      "+---------------------------+-----+\n",
      "|BlobPreflightRequest       |84   |\n",
      "|ListFilesystemDir          |393  |\n",
      "|GetBlobServiceProperties   |55   |\n",
      "|GetBlobMetadata            |7    |\n",
      "|GetBlob                    |664  |\n",
      "|ListBlobs                  |3148 |\n",
      "|GetPathAccessControl       |6    |\n",
      "|GetContainerProperties     |1318 |\n",
      "|ListContainers             |793  |\n",
      "|GetPathStatus              |857  |\n",
      "|GetBlockList               |150  |\n",
      "|GetBlobProperties          |647  |\n",
      "|GetContainerServiceMetadata|45   |\n",
      "|ReadFile                   |682  |\n",
      "|GetFilesystemProperties    |3    |\n",
      "|ListFilesystems            |5    |\n",
      "|GetContainerACL            |2    |\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"operationName\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727ade62-6abe-4679-acd1-709cae583375",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Focusing on : time, specific operations and blob name\n",
    "\n",
    "We need to ignore some of the operations and focus only on ```ReadFile``` and ```GetBlob```. We also need to ignore some of the platform related operations.\n",
    "\n",
    "The following cell creates a final_df with 3 fields: time, operation name, blob name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d03010-5d82-4e1f-a07a-254b32018bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, from_unixtime, unix_timestamp\n",
    "\n",
    "# Define the format of your time column\n",
    "time_format = \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSS'Z'\"\n",
    "# Filtering out platform operations\n",
    "filtered_df = df.filter(~(col(\"uri\").contains(\"$accountmetadata\") | \n",
    "                          col(\"uri\").contains(\"$logs\") | \n",
    "                          col(\"uri\").contains(\"%24logs\")))\n",
    "# Filtering out all non related operations\n",
    "filtered_df = filtered_df.filter(col(\"operationName\").isin(\"GetBlob\", \"ReadFile\"))\n",
    "# Regular expression pattern to extract desired blob_name\n",
    "pattern = r'(?:https?:\\/\\/[^\\/]+\\/)([^?]+)'\n",
    "\n",
    "# Extract blob_name using regexp_extract\n",
    "df_with_blob_name = filtered_df.withColumn(\"blob_name\", regexp_extract(col(\"uri\"), pattern, 1))\n",
    "# Convert the string column to datetime\n",
    "df_with_datetime = df_with_blob_name.withColumn(\"time\", from_unixtime(unix_timestamp(col(\"time\"), time_format)))\n",
    "\n",
    "\n",
    "final_df = df_with_datetime.select(\"time\", \"operationName\", \"blob_name\")\n",
    "\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e68c323-9106-4d23-a897-88ab01c1ec37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Researching Access Patterns\n",
    "\n",
    "#### Last access\n",
    "\n",
    "While you can enable last access time on your storage account, it is not enabled by default. The following cell will show you the last access time for each blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601fb0de-1be0-4e9f-bc0f-ae26102aee76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group by blob_name and aggregate to get the latest timestamp for each blob\n",
    "result_df = final_df.groupBy(\"blob_name\").agg(max(\"time\").alias(\"last_access_timestamp\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e1bebf-41fc-4685-9151-63c0c9f6c6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Looking at the inventory \n",
    "\n",
    "The following cell performs several activities:\n",
    "\n",
    "- Reading the blob inventory parquet file\n",
    "- Remove platform related rows\n",
    "- Select required fields ( try not to bring them in the first place )\n",
    "- Convert time to human readable (from ```long```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c1bf39-2b78-46e9-ad2e-6001ca46817c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,from_unixtime\n",
    "# the inventory files are in this container (consider moving later to the lake)\n",
    "container = 'inventory'\n",
    "# Define the path to your parquet files\n",
    "inventory_file = \"parquet-all.parquet\"\n",
    "path = f\"abfss://{container}@{invent_storage_account_name}.dfs.core.windows.net/2023/08/27/07-02-03/parquet-all/{inventory_file}\"\n",
    "\n",
    "# Read the parquet files into a DataFrame\n",
    "_df = spark.read.parquet(path)\n",
    "# first lets filter any platform related rows from the dataframe: Filter out rows where Name starts with \"$logs\"\n",
    "filtered_inventory = _df.filter(~col(\"Name\").startswith(\"$logs\"))\n",
    "# We need only these fields:\n",
    "new_df = filtered_inventory.select(\"Name\",\"Creation-Time\", \"Last-Modified\", \"Content-Length\")\n",
    "# Readable date time format\n",
    "date_format = \"yyyy-MM-dd HH:00:00\"\n",
    "\n",
    "# Convert Creation-Time and Last-Modified to the desired format\n",
    "inventory_df = new_df.withColumn(\"Creation-Time\", from_unixtime(col(\"Creation-Time\") / 1000, date_format))\n",
    "inventory_df = inventory_df.withColumn(\"Last-Modified\", from_unixtime(col(\"Last-Modified\") / 1000, date_format))\n",
    "\n",
    "# Show the result\n",
    "inventory_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b31962-c639-492f-871b-ba8f12b1a9f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining the two data frames \n",
    "\n",
    "__Here's a step-by-step approach:__\n",
    "\n",
    "- Prepare the inventory_df for joining - For each blob, create two rows: one for the creation event and one for the update event, along with their corresponding timestamps.\n",
    "\n",
    "- Join the two DataFrames based on the Name in inventory_df and blob_name in final_df.\n",
    "\n",
    "- Extract the relevant events based on the operationName column and the timestamp difference between Creation-Time and Last-Modified.\n",
    "\n",
    "- Filter and construct the final DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51092d78-3cad-47d8-9920-e2fa110e02ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode, arrays_zip, struct, lit, when,min,max\n",
    "\n",
    "\n",
    "# Create a DataFrame for creation events\n",
    "creation_df = inventory_df.select(\n",
    "    col(\"Name\"),\n",
    "    col(\"Content-Length\"),\n",
    "    col(\"Creation-Time\").alias(\"event_time\"),\n",
    "    lit(\"creation\").alias(\"eventType\")\n",
    ")\n",
    "\n",
    "# Create a DataFrame for update events\n",
    "update_df = inventory_df.select(\n",
    "    col(\"Name\"),\n",
    "    col(\"Content-Length\"),\n",
    "    col(\"Last-Modified\").alias(\"event_time\"),\n",
    "    lit(\"update\").alias(\"eventType\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames using union\n",
    "inventory_expanded = creation_df.union(update_df)\n",
    "\n",
    "# Remove rows that indicate same time for creation and update:\n",
    "# Group by Name and aggregate to check if creation and update times are the same\n",
    "\n",
    "agg_df = inventory_expanded.groupBy(\"Name\", \"Content-Length\")\\\n",
    "                           .agg(min(\"event_time\").alias(\"min_time\"), \n",
    "                                max(\"event_time\").alias(\"max_time\"))\\\n",
    "                           .withColumn(\"eventType\", \n",
    "                                       when(col(\"min_time\") == col(\"max_time\"), lit(\"creation\")).otherwise(lit(\"update\")))\n",
    "\n",
    "\n",
    "# Select the appropriate columns based on the condition\n",
    "_result = agg_df.withColumn(\"event_time\", \n",
    "                            when(col(\"eventType\") == \"creation\", col(\"min_time\"))\n",
    "                            .otherwise(col(\"max_time\"))\n",
    ")\n",
    "\n",
    "_result = _result.select(\"Name\", \"event_time\", \"eventType\", \"Content-Length\")\n",
    "\n",
    "# Join with final_df (we created it in previous step from the logs). The join is done on the blob name.\n",
    "joined_df = final_df.join(_result, final_df.blob_name == _result.Name, how=\"outer\")\n",
    "\n",
    "# If operationName is \"ReadFile\" or \"GetBlob\", update the eventType\n",
    "final_result = joined_df.withColumn(\"eventType\", \n",
    "                                    when(col(\"operationName\").isin([\"ReadFile\", \"GetBlob\"]), col(\"operationName\"))\n",
    "                                    .otherwise(col(\"eventType\"))\n",
    ")\n",
    "\n",
    "# Select the desired columns and order by the name of the blob and the event time\n",
    "final_result = final_result.select(\"event_time\", \"eventType\", \"Name\", \"Content-Length\").orderBy(\"Name\", \"event_time\")\n",
    "\n",
    "final_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666de775-7b63-4f39-92aa-cf81ad660b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct blobs in inventory_df: 83213\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of distinct blobs from inventory_df\n",
    "total_blobs_inventory = inventory_expanded.agg(countDistinct(\"Name\").alias(\"total_blobs_inventory\")).collect()[0][\"total_blobs_inventory\"]\n",
    "\n",
    "# Print the result\n",
    "print(\"Total number of distinct blobs in inventory_df:\", total_blobs_inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54531099-9663-4210-9013-c9eacad8838e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storage behaviour\n",
    "\n",
    "If we examine the dataframe and count the number of events (creation, update, Read, Get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b339ac9-6e5e-46e2-8dda-73247d9dd450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by eventType and count the occurrences\n",
    "grouped_result = final_result.groupBy(\"eventType\").agg(count(\"Name\").alias(\"count\"))\n",
    "\n",
    "# Display the result\n",
    "grouped_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the number of events for each operation type on the storage we were experimenting with:\n",
    "|eventType|count|\n",
    "|---------|-----|\n",
    "|GetBlob  |7    |\n",
    "|creation |83177|\n",
    "|update   |27   |\n",
    "|ReadFile |29   |\n",
    "\n",
    "This indicate clearly that most (99.92%) of the operations are blob creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97fa12c7-7abd-4ed8-8b7d-82cacdca07ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### How many files are accessed\n",
    "\n",
    "The next task is to try and sort the access patterns into bins. One way of doing this is to calculate the time from creation/update to the last access via the logs, this can tell us how many files are being accessed in each period from the creation of the file.\n",
    "\n",
    "\n",
    "- Filter the DataFrame for rows with eventType of either \"Get\" or \"Read\".\n",
    "- Convert the event_time column from string to a timestamp type.\n",
    "- Filter rows where the event_time is older than 30 days from the current date.\n",
    "- Group by eventType and count the distinct Name (i.e., number of unique blobs).\n",
    "\n",
    "The following cells address this. The first just shows how many blobs were accessed in the time period of the logs. The other cells would show how many \"buckets\" or bins and how many blobs we have in each, the last adds the percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7caf3af-0dd8-440d-bd3c-77ed6850e9be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, current_date, datediff, countDistinct\n",
    "\n",
    "# Convert event_time from string to timestamp\n",
    "filtered_df = final_result.withColumn(\"event_time\", to_timestamp(\"event_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Filter rows with eventType of \"Get\" or \"Read\"\n",
    "filtered_df = filtered_df.filter(col(\"eventType\").isin([\"GetBlob\", \"ReadFile\"]))\n",
    "\n",
    "# Filter rows where event_time is older than 30 days from current date (it could be specific or parameter)\n",
    "filtered_df = filtered_df.filter(datediff(current_date(), col(\"event_time\")) > 30)\n",
    "\n",
    "# Group by eventType and count distinct blobs\n",
    "grouped_result = filtered_df.groupBy(\"eventType\").agg(countDistinct(\"Name\").alias(\"num_blobs\"))\n",
    "\n",
    "# Display the result\n",
    "grouped_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are showing minimal access to blobs. The following table shows the number of events for each operation type on the storage we were experimenting with:\n",
    "\n",
    "|eventType|num_blobs|\n",
    "|---------|---------|\n",
    "|GetBlob  |4        |\n",
    "|ReadFile |5        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700b6c25-6a0e-4847-b98b-3547c7ff2726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor,collect_list, countDistinct, datediff, current_date\n",
    "\n",
    "# Assuming last_access_time is the current date for simplicity. \n",
    "# You can replace current_date() with the appropriate column or value if you have a specific last access time.\n",
    "_filtered_df = filtered_df.withColumn(\"days_diff\", datediff(current_date(), col(\"event_time\")))\n",
    "\n",
    "# Bucket the blobs based on the time difference\n",
    "# Using floor function to create buckets. Each bucket will have a range of 5 days (2 days on either side of a central value).\n",
    "_filtered_df = _filtered_df.withColumn(\"bucket\", floor(col(\"days_diff\") / 5))\n",
    "\n",
    "# Count the number of distinct blobs in each bucket\n",
    "bucket_counts = _filtered_df.groupBy(\"bucket\").agg(countDistinct(\"Name\").alias(\"num_blobs\"))\n",
    "\n",
    "# Convert the bucket number to denote the access pattern in days\n",
    "bucket_counts = bucket_counts.withColumn(\"days\", (col(\"bucket\") * 5) + 2) # Adding 2 to get the central value\n",
    "\n",
    "# Select the desired columns and order by days\n",
    "bucket_counts = bucket_counts.select(\"days\", \"num_blobs\").orderBy(\"days\")\n",
    "\n",
    "# Display the result\n",
    "bucket_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ```days``` indicate the number of days from the creation of the blob to the last access. The following table shows the number of events for each operation type on the storage we were experimenting with:\n",
    "\n",
    "|days|num_blobs|\n",
    "|----|---------|\n",
    "|47  |3        |\n",
    "|52  |1        |\n",
    "|57  |1        |\n",
    "|62  |2        |\n",
    "|252 |2        |\n",
    "\n",
    "So there were total number of 9 blobs accessed in the last 252 days. within this period we could see that there are 5 distinct groups of access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3544a1e2-5986-4121-93ba-bff0c5a44ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "# Calculate the total number of distinct blobs from inventory_df\n",
    "total_blobs_inventory = inventory_expanded.agg(countDistinct(\"Name\").alias(\"total_blobs_inventory\")).collect()[0][\"total_blobs_inventory\"]\n",
    "\n",
    "# Add the total_blobs to the bucket_counts DataFrame\n",
    "bucket_counts = bucket_counts.withColumn(\"total_blobs\", lit(total_blobs_inventory))\n",
    "\n",
    "# Calculate the percentage of blobs accessed for each bucket\n",
    "bucket_counts = bucket_counts.withColumn(\"percentage_accessed\", \n",
    "                                         round((col(\"num_blobs\") / col(\"total_blobs\")) * 100, 3).cast(\"double\"))\n",
    "\n",
    "# Display the result\n",
    "bucket_counts.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar look on the same data, with the ratio of each group from the total number of blobs.\n",
    "\n",
    "|days|num_blobs|total_blobs|percentage_accessed|\n",
    "|----|---------|-----------|-------------------|\n",
    "|47  |3        |83213      |0.004              |\n",
    "|52  |1        |83213      |0.001              |\n",
    "|57  |1        |83213      |0.001              |\n",
    "|62  |2        |83213      |0.002              |\n",
    "|252 |2        |83213      |0.002              |\n",
    "\n",
    "While the storage account we were experimenting with is a shallow example on access patterns, we still could observe few common access patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a166eaf4-6494-45f2-8bd6-34221411fd9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outcome\n",
    "\n",
    "The above result would provide few \"buckets\" of blobs that have similar access patterns. It would show also the precentile of the total number of blobs in the storage, this should help in the decsion making when trying to decide what policy should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2963a72-2d00-4029-a6f9-0582473e9092",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Size calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afc80b1-3f33-44a3-baec-92d812b87c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Conversion factor from bytes to MB\n",
    "bytes_to_mb = 1 / (2**20)\n",
    "\n",
    "# Calculate the total size of all blobs in MB\n",
    "total_size_mb = inventory_df.agg((sum(\"Content-Length\") * bytes_to_mb).alias(\"total_size_mb\")).collect()[0][\"total_size_mb\"]\n",
    "\n",
    "# Calculate the total size of accessed blobs in MB\n",
    "accessed_size_mb = filtered_df.agg((sum(\"Content-Length\") * bytes_to_mb).alias(\"accessed_size_mb\")).collect()[0][\"accessed_size_mb\"]\n",
    "\n",
    "# Calculate the size of blobs that were not accessed in MB\n",
    "not_accessed_size_mb = total_size_mb - accessed_size_mb\n",
    "\n",
    "print(f\"Total size of all blobs (MB): {total_size_mb:.2f}\")\n",
    "print(f\"Total size of accessed blobs (MB): {accessed_size_mb:.2f}\")\n",
    "print(f\"Size of blobs not accessed (MB): {not_accessed_size_mb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results from above cell:\n",
    "\n",
    "```\n",
    "Total size of all blobs (MB): 8863.65\n",
    "Total size of accessed blobs (MB): 5019.23\n",
    "Size of blobs not accessed (MB): 3844.42\n",
    "```\n",
    "\n",
    "So while 99.92% of the operation are blob creation, we see that the 9 blobs that were accessed were with larger size. This is a good indication that for better fine tuned blob cycle policies we need to look at size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b376966-7018-4a50-821b-b80d55ad2a51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "new-diag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
