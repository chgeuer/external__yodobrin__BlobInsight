{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d367b10-6527-4c1f-aec1-865236e9b475",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Blob life cycle - Research notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook provides a structured approach to analyze your blob access patterns. By understanding these patterns, you can craft lifecycle policies that effectively reduce storage costs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following data points available in the same storage account that's being used as your data lake:\n",
    "\n",
    "- Blob Inventory Snapshot: This should be in parquet format.\n",
    "- Blob Diagnostic Logs: Specifically, the 'Read' file logs.\n",
    "\n",
    "## Connecting to Your Data Lake\n",
    "\n",
    "In the Python cell provided, you'll need to input the appropriate values for your data lake storage account and key. This will establish a connection to your data lake.\n",
    "\n",
    "This notebook provide key steps to analyze your blob access patterns, with goal to create life cycle policies that would reduce the cost of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd46ecb7-9277-4501-afb3-fe67f560564c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Inventory storage\n",
    "\n",
    "invent_storage_account_name = \"<your data lake storage account>\"\n",
    "invent_storage_account_key = \"< your key>\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.{0}.dfs.core.windows.net\".format(invent_storage_account_name), invent_storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa87182-6ba5-438f-ad91-9484686230b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Diagnostic Logs from Parquet Files\n",
    "\n",
    "In this section, we will load the diagnostic logs, which have been copied via Azure Data Factory, into a DataFrame for analysis.\n",
    "\n",
    "Ensure the data you're reading is organized in a structured format. For this example, we're reading logs for the year 2023, but you can modify the path to suit your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6512032-662e-4706-8be1-01606ec6c062",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lake_container = 'inventorylake'\n",
    "# Set the path to your parquet log files, note that with 'y=2023/*/*/*/*/' you read all logs of 2023\n",
    "diag_log_path = 'diag_logs/resourceId=/subscriptions/11acf2e0-XXXX-XXXX-86ef-37c53a9XXXXX/resourceGroups/ext-lake/providers/Microsoft.Storage/storageAccounts/targetextsa/blobServices/default/y=2023/*/*/*/*/'\n",
    "# Define the path to your parquet files so spark can read them\n",
    "path = f\"abfss://{lake_container}@{invent_storage_account_name}.dfs.core.windows.net/{diag_log_path}\"\n",
    "\n",
    "# Read the parquet files into a DataFrame\n",
    "df = spark.read.parquet(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b9319b-391f-493e-adc0-d75a93dfb98d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Summary of Activities Over the Selected Period\n",
    "\n",
    "To gain a quick overview of the various operations performed in your storage account, we'll employ the groupBy operation. This will help us understand the distribution of different activities.\n",
    "\n",
    "It's crucial to note that for analyzing access patterns, our primary focus will be on the 'GetBlob' and 'ReadFile' operations. By setting truncate=False, we ensure that the complete content of each field is displayed, allowing for a thorough understanding of each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d99164-9310-4816-a3a0-1371f7a66ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group the DataFrame by operation names and count the occurrences of each operation\n",
    "df.groupBy(\"operationName\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results of the groupBy operation on the diagnostic logs our experimental storage account:\n",
    "\n",
    "|operationName              |count|\n",
    "|---------------------------|-----|\n",
    "|BlobPreflightRequest       |84   |\n",
    "|ListFilesystemDir          |393  |\n",
    "|GetBlobServiceProperties   |55   |\n",
    "|GetBlobMetadata            |7    |\n",
    "|GetBlob                    |664  |\n",
    "|ListBlobs                  |3148 |\n",
    "|GetPathAccessControl       |6    |\n",
    "|GetContainerProperties     |1318 |\n",
    "|ListContainers             |793  |\n",
    "|GetPathStatus              |857  |\n",
    "|GetBlockList               |150  |\n",
    "|GetBlobProperties          |647  |\n",
    "|GetContainerServiceMetadata|45   |\n",
    "|ReadFile                   |682  |\n",
    "|GetFilesystemProperties    |3    |\n",
    "|ListFilesystems            |5    |\n",
    "|GetContainerACL            |2    |\n",
    "\n",
    "#### Analysis of Operations:\n",
    "\n",
    "From the table above, we can derive a few insights:\n",
    "\n",
    "High Frequency Operations: The ```ListBlobs``` and ```GetContainerProperties``` operations are the most frequent, with 3148 and 1318 counts respectively. This indicates regular listing and retrieval activities on the blobs and container properties.\n",
    "Primary Access Patterns: The ```GetBlob``` and ```ReadFile``` operations, which are crucial for our analysis, have been performed 664 and 682 times respectively. This shows a balanced distribution of both blob retrieval and file reading activities.\n",
    "Least Frequent Operations: Operations like ```GetFilesystemProperties```, ```ListFilesystems```, and ```GetContainerACL``` have the lowest counts, suggesting they are not regular activities in the current environment.\n",
    "\n",
    "Understanding the distribution of these operations will help in formulating effective lifecycle policies that reflect real usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727ade62-6abe-4679-acd1-709cae583375",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filtering by Time, Specific Operations, and Blob Name\n",
    "\n",
    "For a more targeted analysis, it's essential to narrow down our dataset. We'll focus on the operations ```ReadFile``` and ```GetBlob``` as these are central to understanding blob access patterns. Furthermore, we'll eliminate platform-related operations to ensure that our insights are based solely on user-based activities.\n",
    "\n",
    "In the upcoming cell, we'll generate a new DataFrame (```final_df```) which will consist of three fields:\n",
    "\n",
    "- ```time```: Timestamp of the operation\n",
    "- ```operationName```: The name of the operation (either ```ReadFile``` or ```GetBlob```)\n",
    "- ```blob_name```: The name of the blob being accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d03010-5d82-4e1f-a07a-254b32018bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, from_unixtime, unix_timestamp\n",
    "\n",
    "# Define the format of your time column\n",
    "time_format = \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSS'Z'\"\n",
    "# Filtering out platform operations\n",
    "filtered_df = df.filter(~(col(\"uri\").contains(\"$accountmetadata\") | \n",
    "                          col(\"uri\").contains(\"$logs\") | \n",
    "                          col(\"uri\").contains(\"%24logs\")))\n",
    "# Filtering out all non related operations\n",
    "filtered_df = filtered_df.filter(col(\"operationName\").isin(\"GetBlob\", \"ReadFile\"))\n",
    "# Regular expression pattern to extract desired blob_name\n",
    "pattern = r'(?:https?:\\/\\/[^\\/]+\\/)([^?]+)'\n",
    "\n",
    "# Extract blob_name using regexp_extract\n",
    "df_with_blob_name = filtered_df.withColumn(\"blob_name\", regexp_extract(col(\"uri\"), pattern, 1))\n",
    "# Convert the string column to datetime\n",
    "df_with_datetime = df_with_blob_name.withColumn(\"time\", from_unixtime(unix_timestamp(col(\"time\"), time_format)))\n",
    "\n",
    "# Select the relevant columns for our final DataFrame\n",
    "final_df = df_with_datetime.select(\"time\", \"operationName\", \"blob_name\")\n",
    "\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e68c323-9106-4d23-a897-88ab01c1ec37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyzing Access Patterns\n",
    "\n",
    "#### Determining the Last Access Time\n",
    "\n",
    "Azure Storage does provide an option to track the last access time for blobs, but it's not enabled by default. To bridge this gap, we can derive the last access time for each blob using our diagnostic logs. In the subsequent cell, we'll group the data by each blob and identify the most recent access timestamp for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601fb0de-1be0-4e9f-bc0f-ae26102aee76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group by blob_name and aggregate to get the latest timestamp for each blob\n",
    "result_df = final_df.groupBy(\"blob_name\").agg(max(\"time\").alias(\"last_access_timestamp\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e1bebf-41fc-4685-9151-63c0c9f6c6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Analyzing the Blob Inventory \n",
    "\n",
    "In the next steps, we will focus on reading and processing the blob inventory stored as a parquet file. This process involves:\n",
    "\n",
    "1. Loading the blob inventory data.\n",
    "2. Filtering out platform-related entries.\n",
    "3. Selecting the relevant fields. (For optimization, it's a good practice to only retrieve necessary fields during the initial read.)\n",
    "4. Converting timestamps from the long data type into a more human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c1bf39-2b78-46e9-ad2e-6001ca46817c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,from_unixtime\n",
    "# the inventory files are in this container (consider moving later to the lake)\n",
    "container = 'inventory'\n",
    "# Define the path to your parquet files\n",
    "inventory_file = \"parquet-all.parquet\"\n",
    "path = f\"abfss://{container}@{invent_storage_account_name}.dfs.core.windows.net/2023/08/27/07-02-03/parquet-all/{inventory_file}\"\n",
    "\n",
    "# Read the parquet files into a DataFrame\n",
    "_df = spark.read.parquet(path)\n",
    "# first lets filter any platform related rows from the dataframe: Filter out rows where Name starts with \"$logs\"\n",
    "filtered_inventory = _df.filter(~col(\"Name\").startswith(\"$logs\"))\n",
    "# We need only these fields:\n",
    "new_df = filtered_inventory.select(\"Name\",\"Creation-Time\", \"Last-Modified\", \"Content-Length\")\n",
    "# Readable date time format\n",
    "date_format = \"yyyy-MM-dd HH:00:00\"\n",
    "\n",
    "# Convert Creation-Time and Last-Modified to the desired format\n",
    "inventory_df = new_df.withColumn(\"Creation-Time\", from_unixtime(col(\"Creation-Time\") / 1000, date_format))\n",
    "inventory_df = inventory_df.withColumn(\"Last-Modified\", from_unixtime(col(\"Last-Modified\") / 1000, date_format))\n",
    "\n",
    "# Show the result\n",
    "inventory_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b31962-c639-492f-871b-ba8f12b1a9f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Merging the Inventory and Access Logs \n",
    "\n",
    "To gain a comprehensive view of blob events, we'll merge data from both the blob inventory (```inventory_df```) and the access logs (```final_df```).\n",
    "\n",
    "Approach:\n",
    "\n",
    "1. Expand the ```inventory_df```: For each blob, create separate entries for both the creation and update events, along with their associated timestamps.\n",
    "2. Join DataFrames: Merge ```inventory_df``` and ```final_df``` based on the blob names.\n",
    "3. Extract Relevant Events: Utilize the operationName column and the timestamp difference between ```Creation-Time``` and ```Last-Modified``` to determine the nature of the event (e.g., creation, update, read).\n",
    "4. Filter and Construct the Final DataFrame: Present the results in a structured manner, highlighting the event type, blob name, and associated timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51092d78-3cad-47d8-9920-e2fa110e02ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode, arrays_zip, struct, lit, when,min,max\n",
    "\n",
    "\n",
    "# Create a DataFrame for creation events\n",
    "creation_df = inventory_df.select(\n",
    "    col(\"Name\"),\n",
    "    col(\"Content-Length\"),\n",
    "    col(\"Creation-Time\").alias(\"event_time\"),\n",
    "    lit(\"creation\").alias(\"eventType\")\n",
    ")\n",
    "\n",
    "# Create a DataFrame for update events\n",
    "update_df = inventory_df.select(\n",
    "    col(\"Name\"),\n",
    "    col(\"Content-Length\"),\n",
    "    col(\"Last-Modified\").alias(\"event_time\"),\n",
    "    lit(\"update\").alias(\"eventType\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames using union\n",
    "inventory_expanded = creation_df.union(update_df)\n",
    "\n",
    "# Remove rows that indicate same time for creation and update:\n",
    "# Group by Name and aggregate to check if creation and update times are the same\n",
    "\n",
    "agg_df = inventory_expanded.groupBy(\"Name\", \"Content-Length\")\\\n",
    "                           .agg(min(\"event_time\").alias(\"min_time\"), \n",
    "                                max(\"event_time\").alias(\"max_time\"))\\\n",
    "                           .withColumn(\"eventType\", \n",
    "                                       when(col(\"min_time\") == col(\"max_time\"), lit(\"creation\")).otherwise(lit(\"update\")))\n",
    "\n",
    "\n",
    "# Select the appropriate columns based on the condition\n",
    "_result = agg_df.withColumn(\"event_time\", \n",
    "                            when(col(\"eventType\") == \"creation\", col(\"min_time\"))\n",
    "                            .otherwise(col(\"max_time\"))\n",
    ")\n",
    "\n",
    "_result = _result.select(\"Name\", \"event_time\", \"eventType\", \"Content-Length\")\n",
    "\n",
    "# Join with final_df (we created it in previous step from the logs). The join is done on the blob name.\n",
    "joined_df = final_df.join(_result, final_df.blob_name == _result.Name, how=\"outer\")\n",
    "\n",
    "# If operationName is \"ReadFile\" or \"GetBlob\", update the eventType\n",
    "final_result = joined_df.withColumn(\"eventType\", \n",
    "                                    when(col(\"operationName\").isin([\"ReadFile\", \"GetBlob\"]), col(\"operationName\"))\n",
    "                                    .otherwise(col(\"eventType\"))\n",
    ")\n",
    "\n",
    "# Select the desired columns and order by the name of the blob and the event time\n",
    "final_result = final_result.select(\"event_time\", \"eventType\", \"Name\", \"Content-Length\").orderBy(\"Name\", \"event_time\")\n",
    "\n",
    "final_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Distinct Blobs in the Inventory\n",
    "\n",
    "To understand the scale of our data, let's determine the total number of unique blobs present in our inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666de775-7b63-4f39-92aa-cf81ad660b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct blobs in inventory_df: 83213\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of distinct blobs from inventory_df\n",
    "total_blobs_inventory = inventory_expanded.agg(countDistinct(\"Name\").alias(\"total_blobs_inventory\")).collect()[0][\"total_blobs_inventory\"]\n",
    "\n",
    "# Print the result\n",
    "print(\"Total number of distinct blobs in inventory_df:\", total_blobs_inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54531099-9663-4210-9013-c9eacad8838e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyzing Storage Behavior\n",
    "\n",
    "By grouping and counting events in the ```final_result``` DataFrame, we can gain insights into the predominant actions taken on the storage. Let's break down the number of each event type, including creation, update, and read actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b339ac9-6e5e-46e2-8dda-73247d9dd450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by eventType and count the occurrences\n",
    "grouped_result = final_result.groupBy(\"eventType\").agg(count(\"Name\").alias(\"count\"))\n",
    "\n",
    "# Display the result\n",
    "grouped_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the distribution of different event types for our experimental storage:\n",
    "\n",
    "|eventType|count|\n",
    "|---------|-----|\n",
    "|GetBlob  |7    |\n",
    "|creation |83177|\n",
    "|update   |27   |\n",
    "|ReadFile |29   |\n",
    "\n",
    "From the results, it's evident that the dominant operation on this storage is blob creation, accounting for __99.92%__ of all events. This highlights a potential area for optimization, as understanding such patterns can inform better storage management strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97fa12c7-7abd-4ed8-8b7d-82cacdca07ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyzing File Access Patterns\n",
    "\n",
    "Understanding the frequency and timing of file access can be crucial for optimizing storage costs and management. One insightful way to analyze this is by calculating the duration from when a file was created (or last updated) to its most recent access time. This can help identify patterns of blob usage over time.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Filter the dataset to consider only the \"GetBlob\" and \"ReadFile\" events.\n",
    "2. Convert the event time from a string to an actual timestamp.\n",
    "3. Exclude blobs accessed within the last 30 days (as they are recent and may not reflect longer-term patterns).\n",
    "4. Group the data by event type and count the distinct blobs accessed for each.\n",
    "\n",
    "\n",
    "Let's start by calculating how many blobs were accessed during the period covered by our logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7caf3af-0dd8-440d-bd3c-77ed6850e9be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, current_date, datediff, countDistinct\n",
    "\n",
    "# Convert event_time from string to timestamp\n",
    "filtered_df = final_result.withColumn(\"event_time\", to_timestamp(\"event_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Filter rows with eventType of \"Get\" or \"Read\"\n",
    "filtered_df = filtered_df.filter(col(\"eventType\").isin([\"GetBlob\", \"ReadFile\"]))\n",
    "\n",
    "# Filter rows where event_time is older than 30 days from current date (it could be specific or parameter)\n",
    "filtered_df = filtered_df.filter(datediff(current_date(), col(\"event_time\")) > 30)\n",
    "\n",
    "# Group by eventType and count distinct blobs\n",
    "grouped_result = filtered_df.groupBy(\"eventType\").agg(countDistinct(\"Name\").alias(\"num_blobs\"))\n",
    "\n",
    "# Display the result\n",
    "grouped_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Results:\n",
    "\n",
    "From the data, it's evident that there's limited access to the blobs over the extended period covered by our logs:\n",
    "\n",
    "|eventType|num_blobs|\n",
    "|---------|---------|\n",
    "|GetBlob  |4        |\n",
    "|ReadFile |5        |\n",
    "\n",
    "This indicates that a majority of blobs, once created or updated, aren't accessed frequently. Such insights can be crucial when deciding on blob lifecycle policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Blob Access into Bins\n",
    "\n",
    "To further refine our understanding of blob access patterns, we can group blobs into bins based on the number of days since their creation or last update. This will allow us to see if there are common periods after which blobs are accessed.\n",
    "\n",
    "In the next cell:\n",
    "\n",
    "1. We compute the difference between the current date and each blob's event time.\n",
    "2. We group blobs into 5-day bins (e.g., 0-4 days, 5-9 days, etc.).\n",
    "3. We count the number of unique blobs in each bin.\n",
    "\n",
    "\n",
    "Let's see how the blobs are distributed across these bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700b6c25-6a0e-4847-b98b-3547c7ff2726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor,collect_list, countDistinct, datediff, current_date\n",
    "\n",
    "# Assuming last_access_time is the current date for simplicity. \n",
    "# You can replace current_date() with the appropriate column or value if you have a specific last access time.\n",
    "_filtered_df = filtered_df.withColumn(\"days_diff\", datediff(current_date(), col(\"event_time\")))\n",
    "\n",
    "# Bucket the blobs based on the time difference\n",
    "# Using floor function to create buckets. Each bucket will have a range of 5 days (2 days on either side of a central value).\n",
    "_filtered_df = _filtered_df.withColumn(\"bucket\", floor(col(\"days_diff\") / 5))\n",
    "\n",
    "# Count the number of distinct blobs in each bucket\n",
    "bucket_counts = _filtered_df.groupBy(\"bucket\").agg(countDistinct(\"Name\").alias(\"num_blobs\"))\n",
    "\n",
    "# Convert the bucket number to denote the access pattern in days\n",
    "bucket_counts = bucket_counts.withColumn(\"days\", (col(\"bucket\") * 5) + 2) # Adding 2 to get the central value\n",
    "\n",
    "# Select the desired columns and order by days\n",
    "bucket_counts = bucket_counts.select(\"days\", \"num_blobs\").orderBy(\"days\")\n",
    "\n",
    "# Display the result\n",
    "bucket_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Blob Access Bins\n",
    "\n",
    "The table below depicts the distribution of blob access based on the number of days since their creation or last update:\n",
    "\n",
    "\n",
    "|days|num_blobs|\n",
    "|----|---------|\n",
    "|47  |3        |\n",
    "|52  |1        |\n",
    "|57  |1        |\n",
    "|62  |2        |\n",
    "|252 |2        |\n",
    "\n",
    "__From the table:__\n",
    "\n",
    "- The ```days``` column indicates the number of days since the creation or last update of the blob to its last access.\n",
    "- The ```num_blobs``` column signifies how many blobs fit into each respective time bin.\n",
    "\n",
    "__Insights:__\n",
    "\n",
    "- We observe that a total of 9 blobs were accessed in the last 252 days.\n",
    "- Within this timeframe, there are 5 distinct periods (or bins) when blobs were accessed.\n",
    "\n",
    "This binning approach allows us to understand typical access patterns and could inform decisions on how long to retain blobs in higher-cost storage tiers before moving them to archival or cooler storage, thereby potentially realizing cost savings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Percentage of Blob Access\n",
    "\n",
    "While understanding the distribution of blob access in bins is insightful, it's also important to gauge the proportion of blobs accessed relative to the total number of blobs in storage. By calculating this percentage, we can get a better sense of how active or dormant our storage is over a specified timeframe.\n",
    "\n",
    "In the following cell, we will:\n",
    "\n",
    "1. Compute the total number of distinct blobs in the inventory.\n",
    "2. Incorporate this total into our previously created bin distribution table.\n",
    "3. Calculate the percentage of blobs accessed for each bin, providing a clearer perspective on the intensity of access.\n",
    "\n",
    "Let's view the distribution as a percentage of the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3544a1e2-5986-4121-93ba-bff0c5a44ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "# Calculate the total number of distinct blobs from inventory_df\n",
    "total_blobs_inventory = inventory_expanded.agg(countDistinct(\"Name\").alias(\"total_blobs_inventory\")).collect()[0][\"total_blobs_inventory\"]\n",
    "\n",
    "# Add the total_blobs to the bucket_counts DataFrame\n",
    "bucket_counts = bucket_counts.withColumn(\"total_blobs\", lit(total_blobs_inventory))\n",
    "\n",
    "# Calculate the percentage of blobs accessed for each bucket\n",
    "bucket_counts = bucket_counts.withColumn(\"percentage_accessed\", \n",
    "                                         round((col(\"num_blobs\") / col(\"total_blobs\")) * 100, 3).cast(\"double\"))\n",
    "\n",
    "# Display the result\n",
    "bucket_counts.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Blob Access as a Percentage:\n",
    "\n",
    "The table below showcases the ratio of each blob access group compared to the total number of blobs:\n",
    "\n",
    "|days|num_blobs|total_blobs|percentage_accessed|\n",
    "|----|---------|-----------|-------------------|\n",
    "|47  |3        |83213      |0.004              |\n",
    "|52  |1        |83213      |0.001              |\n",
    "|57  |1        |83213      |0.001              |\n",
    "|62  |2        |83213      |0.002              |\n",
    "|252 |2        |83213      |0.002              |\n",
    "\n",
    "Although the storage account under investigation provides a limited view of access patterns, it still offers insights into a few common access behaviors. The percentages indicate a relatively low activity in terms of accessing the blobs post their creation or last update, suggesting that most of the data remains dormant over extended periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a166eaf4-6494-45f2-8bd6-34221411fd9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outcome\n",
    "\n",
    "The analysis above has led to the identification of various \"buckets\" or clusters of blobs that exhibit similar access patterns. By discerning the percentage of total blobs in each cluster, we can make more informed decisions regarding the ideal lifecycle policies for data storage.\n",
    "\n",
    "### Size Calculations\n",
    "\n",
    "It's not just the number of blobs that's important; the size of these blobs can also have a significant impact on storage costs and decisions about data lifecycle management. In this section, we'll delve into the size of the blobs to discern the overall storage footprint, both for accessed and non-accessed blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afc80b1-3f33-44a3-baec-92d812b87c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Conversion factor from bytes to MB\n",
    "bytes_to_mb = 1 / (2**20)\n",
    "\n",
    "# Calculate the total size of all blobs in MB\n",
    "total_size_mb = inventory_df.agg((sum(\"Content-Length\") * bytes_to_mb).alias(\"total_size_mb\")).collect()[0][\"total_size_mb\"]\n",
    "\n",
    "# Calculate the total size of accessed blobs in MB\n",
    "accessed_size_mb = filtered_df.agg((sum(\"Content-Length\") * bytes_to_mb).alias(\"accessed_size_mb\")).collect()[0][\"accessed_size_mb\"]\n",
    "\n",
    "# Calculate the size of blobs that were not accessed in MB\n",
    "not_accessed_size_mb = total_size_mb - accessed_size_mb\n",
    "\n",
    "print(f\"Total size of all blobs (MB): {total_size_mb:.2f}\")\n",
    "print(f\"Total size of accessed blobs (MB): {accessed_size_mb:.2f}\")\n",
    "print(f\"Size of blobs not accessed (MB): {not_accessed_size_mb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights on Blob Sizes\n",
    "\n",
    "These are the results from above cell:\n",
    "\n",
    "```\n",
    "Total size of all blobs (MB): 8863.65\n",
    "Total size of accessed blobs (MB): 5019.23\n",
    "Size of blobs not accessed (MB): 3844.42\n",
    "```\n",
    "\n",
    "Despite the fact that 99.92% of operations are related to blob creation, we observe that the few blobs accessed tend to be of a larger size. This underscores the importance of considering both frequency of access and the actual size of the blobs when devising lifecycle policies. Specifically, it suggests that size-based criteria could be a pivotal factor in fine-tuning blob lifecycle management strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b376966-7018-4a50-821b-b80d55ad2a51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Our analysis has illuminated the access patterns of blobs in the storage account and highlighted the significance of considering blob sizes. Armed with this knowledge, organizations can craft more efficient storage strategies, potentially leading to cost savings and optimized data management. As storage requirements evolve and data grows, continually revisiting and refining these strategies will ensure that storage is both cost-effective and aligned with operational needs.\n",
    "\n",
    "Remember, while this analysis provides a foundation, real-world scenarios may require deeper dives and considerations tailored to specific use cases. Always adapt and iterate based on the unique characteristics of your data environment."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "new-diag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
